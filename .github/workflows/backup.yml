name: Database Backup

on:
  # Run automatically every day at 2 AM UTC (4 AM Israel time)
  schedule:
    - cron: '0 2 * * *'
  # Allow manual trigger from GitHub Actions UI
  workflow_dispatch:
  # Run on push to main (optional - for testing)
  push:
    branches:
      - main
    paths:
      - '.github/workflows/backup.yml'

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Create backup directory
        run: mkdir -p backups

      - name: Backup database with pg_dump
        env:
          DATABASE_URL: ${{ secrets.SUPABASE_DATABASE_URL }}
        run: |
          set -e  # Exit on error
          timestamp=$(date +%Y%m%d_%H%M%S)
          backup_file="backups/backup_${timestamp}.sql"
          
          echo "Starting database backup..."
          echo "⚠️  IMPORTANT: Supabase blocks direct connections from GitHub Actions"
          echo "Using Connection Pooling (port 6543) with Session mode"
          
          # Check if URL already uses pooling (port 6543)
          if echo "$DATABASE_URL" | grep -q ":6543"; then
            echo "✓ Connection Pooling URL detected"
            POOLING_URL="$DATABASE_URL"
          else
            echo "Converting direct connection to Connection Pooling..."
            # Convert direct connection (port 5432) to pooling (port 6543)
            # Format: postgresql://postgres:pass@db.xxx.supabase.co:5432/postgres
            # To:     postgresql://postgres:pass@db.xxx.supabase.co:6543/postgres?pgbouncer=true
            POOLING_URL=$(echo "$DATABASE_URL" | sed 's/:5432\//:6543\//' | sed 's/?.*$//')
            # Add pgbouncer=true if not present
            if ! echo "$POOLING_URL" | grep -q "pgbouncer=true"; then
              if echo "$POOLING_URL" | grep -q "?"; then
                POOLING_URL="${POOLING_URL}&pgbouncer=true"
              else
                POOLING_URL="${POOLING_URL}?pgbouncer=true"
              fi
            fi
            echo "Converted URL (password hidden for security)"
          fi
          
          echo "Connecting via Connection Pooling (Session mode)..."
          echo "If this fails, make sure you're using Connection Pooling URL from Supabase Dashboard"
          
          # Try with --no-owner and --no-acl for better compatibility with pooling
          if pg_dump "$POOLING_URL" --no-owner --no-acl > "$backup_file"; then
            echo "✓ Database dump created successfully"
            
            # Get file size
            file_size=$(du -h "$backup_file" | cut -f1)
            echo "File size: $file_size"
            
            # Compress the backup
            echo "Compressing backup..."
            if gzip "$backup_file"; then
              echo "✓ Backup compressed successfully"
              compressed_size=$(du -h "${backup_file}.gz" | cut -f1)
              echo "Compressed size: $compressed_size"
            else
              echo "✗ Compression failed"
              exit 1
            fi
          else
            echo "✗ Database dump failed"
            exit 1
          fi
          
          echo "Backup created: ${backup_file}.gz"
          ls -lh backups/

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Google Drive dependencies
        run: |
          pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib

      - name: Upload to Google Drive
        env:
          GOOGLE_SERVICE_ACCOUNT: ${{ secrets.GOOGLE_SERVICE_ACCOUNT }}
          GOOGLE_DRIVE_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        run: |
          python3 << 'EOF'
          import os
          import json
          import glob
          from google.oauth2 import service_account
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload
          
          # Get credentials from secret
          service_account_info = json.loads(os.getenv('GOOGLE_SERVICE_ACCOUNT'))
          folder_id = os.getenv('GOOGLE_DRIVE_FOLDER_ID')
          
          # Authenticate
          credentials = service_account.Credentials.from_service_account_info(
              service_account_info,
              scopes=['https://www.googleapis.com/auth/drive.file']
          )
          
          # Build Drive service
          drive_service = build('drive', 'v3', credentials=credentials)
          
          # Find all backup files
          backup_files = glob.glob('backups/*.sql.gz')
          
          if not backup_files:
              print("No backup files found to upload")
              exit(1)
          
          # Upload each backup file
          for backup_file in backup_files:
              file_name = os.path.basename(backup_file)
              print(f"Uploading {file_name} to Google Drive...")
              
              file_metadata = {
                  'name': file_name,
                  'parents': [folder_id] if folder_id else []
              }
              
              media = MediaFileUpload(backup_file, mimetype='application/gzip', resumable=True)
              
              file = drive_service.files().create(
                  body=file_metadata,
                  media_body=media,
                  fields='id, name, size'
              ).execute()
              
              print(f"✅ Successfully uploaded {file_name} (ID: {file.get('id')})")
          
          print("✅ All backups uploaded to Google Drive successfully!")
          EOF
        continue-on-error: true  # Continue even if Google Drive upload fails

      - name: Cleanup old backups (keep last 30 days)
        if: success() || failure()  # Run even if previous step failed
        run: |
          # Keep only backups from last 30 days
          find backups/ -name "backup_*.sql.gz" -mtime +30 -delete 2>/dev/null || true
          echo "Cleaned up old backups"

      - name: Create backup summary
        run: |
          echo "## Database Backup Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Backup Files:**" >> $GITHUB_STEP_SUMMARY
          ls -lh backups/ | tail -n +2 | awk '{print "- " $9 " (" $5 ")"}' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ Backup completed successfully!" >> $GITHUB_STEP_SUMMARY

      - name: Upload backup as artifact (for download)
        uses: actions/upload-artifact@v4
        with:
          name: database-backup
          path: backups/*.sql.gz
          retention-days: 7
