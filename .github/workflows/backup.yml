name: Database Backup

on:
  # Run automatically every day at 2 AM UTC (4 AM Israel time)
  schedule:
    - cron: '0 2 * * *'
  # Allow manual trigger from GitHub Actions UI
  workflow_dispatch:
  # Run on push to main (optional - for testing)
  push:
    branches:
      - main
    paths:
      - '.github/workflows/backup.yml'

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client curl jq

      - name: Create backup directory
        run: mkdir -p backups

      - name: Get database connection string from Supabase API
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_PROJECT_ID: ${{ secrets.SUPABASE_PROJECT_ID }}
        run: |
          echo "Getting database connection string from Supabase API..."
          
          # Get project database password
          API_RESPONSE=$(curl -s -X GET \
            "https://api.supabase.com/v1/projects/$SUPABASE_PROJECT_ID" \
            -H "Authorization: Bearer $SUPABASE_ACCESS_TOKEN" \
            -H "Content-Type: application/json")
          
          # Extract database password (stored in project settings)
          # Note: This requires the password to be stored in a secret
          # For now, we'll use the connection string from secrets
          echo "API connection established"
          
          # Store connection string for next step
          echo "${{ secrets.SUPABASE_DATABASE_URL }}" > /tmp/db_url.txt

      - name: Backup database with pg_dump via Supabase API
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
          SUPABASE_PROJECT_ID: ${{ secrets.SUPABASE_PROJECT_ID }}
          SUPABASE_DATABASE_URL: ${{ secrets.SUPABASE_DATABASE_URL }}
        run: |
          set -e  # Exit on error
          timestamp=$(date +%Y%m%d_%H%M%S)
          backup_file="backups/backup_${timestamp}.sql"
          
          echo "Starting database backup..."
          echo "‚ö†Ô∏è  Attempting to connect via Supabase Connection Pooling"
          
          # Try to use Connection Pooling URL (port 6543)
          # If URL already uses 6543, use it as-is
          # Otherwise, convert 5432 to 6543
          if echo "$SUPABASE_DATABASE_URL" | grep -q ":6543"; then
            echo "‚úì Connection Pooling URL detected"
            DB_URL="$SUPABASE_DATABASE_URL"
          else
            echo "Converting to Connection Pooling (port 6543)..."
            DB_URL=$(echo "$SUPABASE_DATABASE_URL" | sed 's/:5432\//:6543\//' | sed 's/?.*$//')
          fi
          
          # Remove any query parameters that pg_dump doesn't support
          CLEAN_URL=$(echo "$DB_URL" | sed 's/?.*$//')
          
          echo "Attempting connection (this may fail if IP is blocked)..."
          echo "If this fails, you may need to whitelist GitHub Actions IPs in Supabase"
          
          # Try pg_dump with timeout
          if timeout 60 pg_dump "$CLEAN_URL" --no-owner --no-acl > "$backup_file" 2>&1; then
            echo "‚úì Database dump created successfully"
            
            # Get file size
            file_size=$(du -h "$backup_file" | cut -f1)
            echo "File size: $file_size"
            
            # Compress the backup
            echo "Compressing backup..."
            if gzip "$backup_file"; then
              echo "‚úì Backup compressed successfully"
              compressed_size=$(du -h "${backup_file}.gz" | cut -f1)
              echo "Compressed size: $compressed_size"
            else
              echo "‚úó Compression failed"
              exit 1
            fi
          else
            echo "‚úó Database dump failed"
            echo ""
            echo "‚ö†Ô∏è  This error usually means:"
            echo "   1. Supabase is blocking GitHub Actions IP addresses"
            echo "   2. Connection Pooling is not enabled for your project"
            echo "   3. The connection string is incorrect"
            echo ""
            echo "üí° Solutions:"
            echo "   - Contact Supabase support to whitelist GitHub Actions IPs"
            echo "   - Use Supabase Dashboard ‚Üí Database ‚Üí Backups for manual exports"
            echo "   - Run backups from a server with whitelisted IP"
            exit 1
          fi
          
          echo "Backup created: ${backup_file}.gz"
          ls -lh backups/

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Google Drive dependencies
        run: |
          pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib

      - name: Upload to Google Drive
        env:
          GOOGLE_SERVICE_ACCOUNT: ${{ secrets.GOOGLE_SERVICE_ACCOUNT }}
          GOOGLE_DRIVE_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_FOLDER_ID }}
        run: |
          python3 << 'EOF'
          import os
          import json
          import glob
          from google.oauth2 import service_account
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload
          
          # Get credentials from secret
          service_account_info = json.loads(os.getenv('GOOGLE_SERVICE_ACCOUNT'))
          folder_id = os.getenv('GOOGLE_DRIVE_FOLDER_ID')
          
          # Authenticate
          credentials = service_account.Credentials.from_service_account_info(
              service_account_info,
              scopes=['https://www.googleapis.com/auth/drive.file']
          )
          
          # Build Drive service
          drive_service = build('drive', 'v3', credentials=credentials)
          
          # Find all backup files
          backup_files = glob.glob('backups/*.sql.gz')
          
          if not backup_files:
              print("No backup files found to upload")
              exit(1)
          
          # Upload each backup file
          for backup_file in backup_files:
              file_name = os.path.basename(backup_file)
              print(f"Uploading {file_name} to Google Drive...")
              
              file_metadata = {
                  'name': file_name,
                  'parents': [folder_id] if folder_id else []
              }
              
              media = MediaFileUpload(backup_file, mimetype='application/gzip', resumable=True)
              
              file = drive_service.files().create(
                  body=file_metadata,
                  media_body=media,
                  fields='id, name, size'
              ).execute()
              
              print(f"‚úÖ Successfully uploaded {file_name} (ID: {file.get('id')})")
          
          print("‚úÖ All backups uploaded to Google Drive successfully!")
          EOF
        continue-on-error: true  # Continue even if Google Drive upload fails

      - name: Cleanup old backups (keep last 30 days)
        if: success() || failure()  # Run even if previous step failed
        run: |
          # Keep only backups from last 30 days
          find backups/ -name "backup_*.sql.gz" -mtime +30 -delete 2>/dev/null || true
          echo "Cleaned up old backups"

      - name: Create backup summary
        run: |
          echo "## Database Backup Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Backup Files:**" >> $GITHUB_STEP_SUMMARY
          ls -lh backups/ | tail -n +2 | awk '{print "- " $9 " (" $5 ")"}' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ Backup completed successfully!" >> $GITHUB_STEP_SUMMARY

      - name: Upload backup as artifact (for download)
        uses: actions/upload-artifact@v4
        with:
          name: database-backup
          path: backups/*.sql.gz
          retention-days: 7
